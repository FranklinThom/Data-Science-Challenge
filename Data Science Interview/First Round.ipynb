{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Exploring The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('DataSet2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1700, 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>x11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>-0.375866</td>\n",
       "      <td>0.427942</td>\n",
       "      <td>-0.922338</td>\n",
       "      <td>0.210758</td>\n",
       "      <td>0.109015</td>\n",
       "      <td>0.621001</td>\n",
       "      <td>-0.444421</td>\n",
       "      <td>0.089970</td>\n",
       "      <td>-0.707711</td>\n",
       "      <td>0.473700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>b</td>\n",
       "      <td>0.047819</td>\n",
       "      <td>0.115627</td>\n",
       "      <td>-1.781739</td>\n",
       "      <td>-0.272785</td>\n",
       "      <td>0.392783</td>\n",
       "      <td>1.094168</td>\n",
       "      <td>-0.975254</td>\n",
       "      <td>-0.353424</td>\n",
       "      <td>0.145543</td>\n",
       "      <td>-0.064961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>d</td>\n",
       "      <td>0.372868</td>\n",
       "      <td>-0.263291</td>\n",
       "      <td>-1.120545</td>\n",
       "      <td>-0.773828</td>\n",
       "      <td>0.830072</td>\n",
       "      <td>-1.727836</td>\n",
       "      <td>1.323876</td>\n",
       "      <td>-1.587291</td>\n",
       "      <td>-0.024916</td>\n",
       "      <td>0.082491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>c</td>\n",
       "      <td>0.059598</td>\n",
       "      <td>0.270797</td>\n",
       "      <td>0.961795</td>\n",
       "      <td>-1.804197</td>\n",
       "      <td>2.931330</td>\n",
       "      <td>1.891656</td>\n",
       "      <td>0.094252</td>\n",
       "      <td>-0.873467</td>\n",
       "      <td>-1.217680</td>\n",
       "      <td>-1.848046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>d</td>\n",
       "      <td>0.616319</td>\n",
       "      <td>0.291275</td>\n",
       "      <td>-1.113519</td>\n",
       "      <td>0.626864</td>\n",
       "      <td>-0.287989</td>\n",
       "      <td>-0.842649</td>\n",
       "      <td>-0.947257</td>\n",
       "      <td>1.198215</td>\n",
       "      <td>0.972420</td>\n",
       "      <td>-1.054313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   y x1        x2        x3        x4        x5        x6        x7        x8  \\\n",
       "0  0  a -0.375866  0.427942 -0.922338  0.210758  0.109015  0.621001 -0.444421   \n",
       "1  0  b  0.047819  0.115627 -1.781739 -0.272785  0.392783  1.094168 -0.975254   \n",
       "2  1  d  0.372868 -0.263291 -1.120545 -0.773828  0.830072 -1.727836  1.323876   \n",
       "3  0  c  0.059598  0.270797  0.961795 -1.804197  2.931330  1.891656  0.094252   \n",
       "4  1  d  0.616319  0.291275 -1.113519  0.626864 -0.287989 -0.842649 -0.947257   \n",
       "\n",
       "         x9       x10       x11  \n",
       "0  0.089970 -0.707711  0.473700  \n",
       "1 -0.353424  0.145543 -0.064961  \n",
       "2 -1.587291 -0.024916  0.082491  \n",
       "3 -0.873467 -1.217680 -1.848046  \n",
       "4  1.198215  0.972420 -1.054313  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head() #First 5 rows in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>x11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1695</th>\n",
       "      <td>1</td>\n",
       "      <td>c</td>\n",
       "      <td>0.541717</td>\n",
       "      <td>0.072494</td>\n",
       "      <td>-1.571099</td>\n",
       "      <td>0.241235</td>\n",
       "      <td>-1.694391</td>\n",
       "      <td>-0.523919</td>\n",
       "      <td>0.440969</td>\n",
       "      <td>-0.099387</td>\n",
       "      <td>1.268869</td>\n",
       "      <td>1.010457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1696</th>\n",
       "      <td>0</td>\n",
       "      <td>c</td>\n",
       "      <td>-0.525727</td>\n",
       "      <td>0.408316</td>\n",
       "      <td>-1.175430</td>\n",
       "      <td>2.058188</td>\n",
       "      <td>1.197211</td>\n",
       "      <td>-1.303748</td>\n",
       "      <td>-0.440300</td>\n",
       "      <td>0.271271</td>\n",
       "      <td>0.339463</td>\n",
       "      <td>-0.272108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1697</th>\n",
       "      <td>0</td>\n",
       "      <td>c</td>\n",
       "      <td>-0.731380</td>\n",
       "      <td>0.164950</td>\n",
       "      <td>-0.477654</td>\n",
       "      <td>-0.057856</td>\n",
       "      <td>0.598471</td>\n",
       "      <td>-1.114582</td>\n",
       "      <td>0.005753</td>\n",
       "      <td>2.407644</td>\n",
       "      <td>0.239817</td>\n",
       "      <td>1.477723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1698</th>\n",
       "      <td>0</td>\n",
       "      <td>c</td>\n",
       "      <td>-0.448287</td>\n",
       "      <td>0.359409</td>\n",
       "      <td>0.983409</td>\n",
       "      <td>-1.057294</td>\n",
       "      <td>-0.501924</td>\n",
       "      <td>0.808380</td>\n",
       "      <td>-1.330170</td>\n",
       "      <td>3.595073</td>\n",
       "      <td>-0.716719</td>\n",
       "      <td>-0.891881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1699</th>\n",
       "      <td>0</td>\n",
       "      <td>b</td>\n",
       "      <td>-0.778458</td>\n",
       "      <td>-0.260288</td>\n",
       "      <td>1.818800</td>\n",
       "      <td>-0.224182</td>\n",
       "      <td>-0.311970</td>\n",
       "      <td>-1.933239</td>\n",
       "      <td>-1.097035</td>\n",
       "      <td>-0.158764</td>\n",
       "      <td>-1.003412</td>\n",
       "      <td>1.239610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      y x1        x2        x3        x4        x5        x6        x7  \\\n",
       "1695  1  c  0.541717  0.072494 -1.571099  0.241235 -1.694391 -0.523919   \n",
       "1696  0  c -0.525727  0.408316 -1.175430  2.058188  1.197211 -1.303748   \n",
       "1697  0  c -0.731380  0.164950 -0.477654 -0.057856  0.598471 -1.114582   \n",
       "1698  0  c -0.448287  0.359409  0.983409 -1.057294 -0.501924  0.808380   \n",
       "1699  0  b -0.778458 -0.260288  1.818800 -0.224182 -0.311970 -1.933239   \n",
       "\n",
       "            x8        x9       x10       x11  \n",
       "1695  0.440969 -0.099387  1.268869  1.010457  \n",
       "1696 -0.440300  0.271271  0.339463 -0.272108  \n",
       "1697  0.005753  2.407644  0.239817  1.477723  \n",
       "1698 -1.330170  3.595073 -0.716719 -0.891881  \n",
       "1699 -1.097035 -0.158764 -1.003412  1.239610  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail() #Last 5 rows in the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### An initial look at the dataset reveals that it consists of 11 independent features, and 1 dependent feature. 10 of the independent features are numerical whereas one of them is a categorical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a', 'b', 'd', 'c'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.x1.unique() #The unique values of x1 feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x1=pd.get_dummies(df['x1'],drop_first=True) #one hot encoding the categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1000\n",
       "1     700\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking The count of each class in the target feature\n",
    "df['y'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAVQ0lEQVR4nO3df/BddX3n8eeLhJ8qFZqAkIDBmmJDu1vbLGXV7TLFFmzV0HaxYZcSW2bj7qClW2cdcB21SlpmV1vdFnQZVELrwGSVFWy1LU11HVcxDcouAiJZqSQlkgAiYF0g8b1/nJN6+fLN93Pz43vvN7nPx8yde87nfM4973P5cl45n3PvuakqJEmaySHjLkCSNPcZFpKkJsNCktRkWEiSmgwLSVKTYSFJajIsdNBKcmaSLSPa1vFJPpfk8STvHcU2pVEyLDSUJP86ycYkTyTZmuTTSV4xgu1WkhfPsPz1SXb2dT2W5PYkr96L7Vyb5PJ9KHU18BBwdFW9ecprf7qv74kkTyd5amD+g/uwzb2S5NYkF8yw/CX9+/7EwGPDftjuFUmu2dfX0XjMH3cBmvuS/A5wKfDvgL8EngLOAVYAnx9jabt8sapekeQQ4GJgXZLFI67hhcBdNc23XKvqVbumk1wLbKmqt+3NRvp9pKq+v5d1DmtnVT13lrexR5LMr6od465jYlWVDx+7fQA/BDwBnDdDn8OB9wEP9I/3AYf3y14PfH5K/wJe3E9fC1wJ/DnwOPAl4Ef6ZZ/r+363r+HXptn2M14feE6/znLgTLoD865lPwZ8FngUuBN4bd++GniaLgSfAD65m/18GfC3wHf655cN7MPg+q+c4b26Frh8SttC4NPAduAR4CbghIHltwLv6t+b/wcsBl4M/K/+PfsL4L8B1wys8y/6/o8CXwZe3re/F9jZv84TwHunqfElwI4Z9uENwD19rX8OLBpY9gFgC/AYsAE4o28/t39/nu63u6Fv/xbwioH1r9i1H7vqAP4tsBn4q5n2zccsHwvGXYCPuf2gO4PYAcyfoc+7+gPacf2B7wvAu/tlzziY921Tw+IR4HS6M92PAjdM13c32/7H1+/Xv6Q/gP4QA2EBHApsAt4KHAb8XN/v1IE6Lp9hO8cC3wZ+vd/O+f38Dw+z/sDrPKsfcDzdWdqRfd03TXkPbgW+AZza78f8/iC5pt+XM+kCdddBdgnwMPBKuqHmX6QLomMGXu+CGWrcbVgAK4G7gR/ta7kc+MzA8guBY/pl/6k/yB/aL/vHIBjo3wqLAq4Bjurfnxn3zcfsPbxmoZYfBh6qmU///w3wrqraVlXbgd+lO6gO68aq2tBv46PAT+5hjWckeZTuwHM+8MtV9Z2pfYDnAldU1VNV9TfAn/X9h/FLwL1V9SdVtaOqrge+BrxmD2t9lqp6sKpuqqrv9XX/PvAvp3S7pqruqaqngRfRnSW9q9+Xz9Kdmeyyiu49/euq+n5VfQq4C/iFPShrXpJHBx5v7NvfQBd2X+9r+V3gFUmO7/fluqr6dr/s9+j+fl60J+/HNN5eVf9QVd/bT/umveA1C7U8DCxojBefCHxzYP6bfduwvjUw/Q90B/U9cWtVtS62nwhsrmeO9X8TWDTkNqbu456uv1tJnge8n+5fy8/vm4+c0m3zlFq2V9WTU5Y/r59+IXB+kvMGlh/Knv032VlVz5+m/YXAB5NcOdC2g25o7MEkl9Gd7b2A7qzgCGAB3bDV3vh+VT0wZfv7um/aC55ZqOWLdOPb587Q5wG6/4l3Oblvg2545KhdC5K8YH8XOKQHgJN2XSDunQz8fT/duv3y1H2cuv6+uJTuYPvPqupoun8lZ0qfwfq2AguTHD7QdtLA9Ga6M5HnDzyeU1V/OM1r7anNwOunvPaRVXVbkp8H3gT8Ml3oHQt8b2BfptvuM/4+6EJm0NR1WvumWWJYaEb9sMjbgSuTnJvkqCSHJnlVkv/cd7seeFuShUkW9P3/tF/2v4HTkvxkkiOAd+5hCQ+y78MY0F0Q/S7wlr7+M+mGkG4YcjufAn60/wjx/CS/BiyjG8raV8+jO6N6tH//Wp+U+jrdENjb+n35WbprS7usBc5LclaSeUmO7Kd3HYj35T39YL/dUwGSHJPkVwf242m6awiH0V3LOmJg3QeBU5IMBuHtdGcK85OcQXftZiatfdMsMSzUVFV/APwO3UFsO92/7t4IfKLvcjmwEfg/wB10F18v79f9Ot1B46+Be9nzj9q+E1jbj5u/bh/24SngtcCr6L4PcRVwYVV9re/yIWBZv51PTLP+w8CrgTfTDc29BXh1VT20tzUNeA/dUM3DdO/Ppxr7UnQXml9Jd5H9rcB/B57sl38D+FW66wkP0Q2XXcIP/n//Q+DCJN8eCPyh9Ndq/hi4McljdAf7n+8Xf5LuE2z/l+6C/EN0fy+73EB3FvFIki/0bW8FfoLuk02X8YPw3t32W/umWZLu707SgSzJTXTXbn5/3LXo4GQaSwegJD+TZEmSQ5K8hm4Y6uZx16WDl5+Gkg5Mi4GP011Evh/4zaq6c7wl6WDmMJQkqclhKElS00E7DLVgwYJasmTJuMuQpAPKbbfd9lBVLZzaftCGxZIlS9i4ceO4y5CkA0qSqXcqAByGkiQNwbCQJDUZFpKkJsNCktRkWEiSmgwLSVLTrIVFkg8n2ZbkqwNtxya5Jcm9/fMxA8suS7IpyT1Jzh5o/+kkd/TL/uuU2xtLkkZgNs8sruWZ99iH7kde1lfVUmB9P0+SZXS3XD6tX+eqJPP6dT4ArAaW9o+prylJmmWzFhZV9TngkSnNK+h+vIT++dyB9huq6smqug/YBJye5ATg6Kr6Yn8P/+uY+RfbJEmzYNTf4D6+qrYCVNXWJMf17YuAWwf6benbnu6np7ZPK8lqurMQTj755H0q9Kf/43X7tL4OTrf9lwvHXYI0FnPlAvd01yFqhvZpVdXVVbW8qpYvXPisW5tIkvbSqMPiwX5oif55W9++hWf+4Pxi4IG+ffE07ZKkERp1WNwMrOqnVwE3DbSvTHJ4klPoLmRv6IesHk9yRv8pqAsH1pEkjcisXbNIcj1wJrAgyRbgHcAVwLokF9H9utd5AFV1Z5J1wF3ADuDiqtrZv9S/p/tk1ZHAp/uHJGmEZi0squr83Sw6azf91wBrpmnfCPz4fixNkrSH5soFbknSHGZYSJKaDAtJUpNhIUlqMiwkSU2GhSSpybCQJDUZFpKkJsNCktRkWEiSmgwLSVKTYSFJajIsJElNhoUkqcmwkCQ1GRaSpCbDQpLUZFhIkpoMC0lSk2EhSWoyLCRJTYaFJKnJsJAkNRkWkqQmw0KS1GRYSJKaDAtJUpNhIUlqMiwkSU2GhSSpybCQJDUZFpKkprGERZL/kOTOJF9Ncn2SI5Icm+SWJPf2z8cM9L8syaYk9yQ5exw1S9IkG3lYJFkE/BawvKp+HJgHrAQuBdZX1VJgfT9PkmX98tOAc4Crkswbdd2SNMnGNQw1HzgyyXzgKOABYAWwtl++Fji3n14B3FBVT1bVfcAm4PQR1ytJE23kYVFVfw+8B7gf2Ap8p6r+Cji+qrb2fbYCx/WrLAI2D7zElr7tWZKsTrIxycbt27fP1i5I0sQZxzDUMXRnC6cAJwLPSXLBTKtM01bTdayqq6tqeVUtX7hw4b4XK0kCxjMM9UrgvqraXlVPAzcCLwMeTHICQP+8re+/BThpYP3FdMNWkqQRGUdY3A+ckeSoJAHOAu4GbgZW9X1WATf10zcDK5McnuQUYCmwYcQ1S9JEmz/qDVbVl5J8DPgysAP4CnA18FxgXZKL6ALlvL7/nUnWAXf1/S+uqp2jrluSJtnIwwKgqt4BvGNK85N0ZxnT9V8DrJntuiRJ0/Mb3JKkJsNCktRkWEiSmgwLSVKTYSFJajIsJElNhoUkqcmwkCQ1GRaSpCbDQpLUZFhIkpoMC0lSk2EhSWoyLCRJTYaFJKlpLL9nIWnf3P+unxh3CZqDTn77HbP22p5ZSJKaDAtJUpNhIUlqMiwkSU2GhSSpybCQJDUZFpKkJsNCktRkWEiSmgwLSVKTYSFJajIsJElNhoUkqcmwkCQ1GRaSpCbDQpLUNJawSPL8JB9L8rUkdyf550mOTXJLknv752MG+l+WZFOSe5KcPY6aJWmSjevM4v3AX1TVS4B/CtwNXAqsr6qlwPp+niTLgJXAacA5wFVJ5o2lakmaUCMPiyRHAz8LfAigqp6qqkeBFcDavtta4Nx+egVwQ1U9WVX3AZuA00dbtSRNtnGcWbwI2A58JMlXklyT5DnA8VW1FaB/Pq7vvwjYPLD+lr5NkjQi4wiL+cBPAR+oqpcC36UfctqNTNNW03ZMVifZmGTj9u3b971SSRIwnrDYAmypqi/18x+jC48Hk5wA0D9vG+h/0sD6i4EHpnvhqrq6qpZX1fKFCxfOSvGSNIlGHhZV9S1gc5JT+6azgLuAm4FVfdsq4KZ++mZgZZLDk5wCLAU2jLBkSZp488e03TcBH01yGPAN4DfogmtdkouA+4HzAKrqziTr6AJlB3BxVe0cT9mSNJmGCosk66vqrFbbsKrqdmD5NIumfb2qWgOs2ZttSZL23YxhkeQI4ChgQf8luV0Xm48GTpzl2iRJc0TrzOINwG/TBcNt/CAsHgOunMW6JElzyIxhUVXvB96f5E1V9UcjqkmSNMcMdc2iqv4oycuAJYPrVNV1s1SXJGkOGfYC958APwLcDuz6JFIBhoUkTYBhPzq7HFhWVdN+c1qSdHAb9kt5XwVeMJuFSJLmrmHPLBYAdyXZADy5q7GqXjsrVUmS5pRhw+Kds1mEJGluG/bTUP9ztguRJM1dw34a6nF+cFvww4BDge9W1dGzVZgkae4Y9szieYPzSc7FX6uTpImxV7cor6pPAD+3n2uRJM1Rww5D/crA7CF037vwOxeSNCGG/TTUawamdwB/B6zY79VIkuakYa9Z/MZsFyJJmruGumaRZHGS/5FkW5IHk3w8yeLZLk6SNDcMe4H7I3S/hX0isAj4ZN8mSZoAw4bFwqr6SFXt6B/XAgtnsS5J0hwybFg8lOSCJPP6xwXAw7NZmCRp7hg2LH4TeB3wLWAr8K8AL3pL0oQY9qOz7wZWVdW3AZIcC7yHLkQkSQe5Yc8s/smuoACoqkeAl85OSZKkuWbYsDgkyTG7Zvozi2HPSiRJB7hhD/jvBb6Q5GN0t/l4HbBm1qqSJM0pw36D+7okG+luHhjgV6rqrlmtTJI0Zww9lNSHgwEhSRNor25RLkmaLIaFJKnJsJAkNRkWkqQmw0KS1GRYSJKaxhYW/d1rv5Lkz/r5Y5PckuTe/nnwG+OXJdmU5J4kZ4+rZkmaVOM8s7gEuHtg/lJgfVUtBdb38yRZBqwETgPOAa5KMm/EtUrSRBtLWPQ/yfpLwDUDzSuAtf30WuDcgfYbqurJqroP2AScPqpaJUnjO7N4H/AW4PsDbcdX1VaA/vm4vn0RsHmg35a+7VmSrE6yMcnG7du37/+qJWlCjTwskrwa2FZVtw27yjRtNV3Hqrq6qpZX1fKFC/3VV0naX8Zxm/GXA69N8ovAEcDRSf4UeDDJCVW1NckJwLa+/xbgpIH1FwMPjLRiSZpwIz+zqKrLqmpxVS2hu3D9N1V1AXAzsKrvtgq4qZ++GViZ5PAkpwBLgQ0jLluSJtpc+gGjK4B1SS4C7gfOA6iqO5Oso7vj7Q7g4qraOb4yJWnyjDUsquqzwGf76YeBs3bTbw3+2JIkjY3f4JYkNRkWkqQmw0KS1GRYSJKaDAtJUpNhIUlqMiwkSU2GhSSpybCQJDUZFpKkJsNCktRkWEiSmgwLSVKTYSFJajIsJElNhoUkqcmwkCQ1GRaSpCbDQpLUZFhIkpoMC0lSk2EhSWoyLCRJTYaFJKnJsJAkNRkWkqQmw0KS1GRYSJKaDAtJUpNhIUlqMiwkSU2GhSSpaeRhkeSkJJ9JcneSO5Nc0rcfm+SWJPf2z8cMrHNZkk1J7kly9qhrlqRJN44zix3Am6vqx4AzgIuTLAMuBdZX1VJgfT9Pv2wlcBpwDnBVknljqFuSJtbIw6KqtlbVl/vpx4G7gUXACmBt320tcG4/vQK4oaqerKr7gE3A6aOtWpIm21ivWSRZArwU+BJwfFVthS5QgOP6bouAzQOrbenbpnu91Uk2Jtm4ffv22SpbkibO2MIiyXOBjwO/XVWPzdR1mraarmNVXV1Vy6tq+cKFC/dHmZIkxhQWSQ6lC4qPVtWNffODSU7ol58AbOvbtwAnDay+GHhgVLVKksbzaagAHwLurqo/GFh0M7Cqn14F3DTQvjLJ4UlOAZYCG0ZVryQJ5o9hmy8Hfh24I8ntfdtbgSuAdUkuAu4HzgOoqjuTrAPuovsk1cVVtXP0ZUvS5Bp5WFTV55n+OgTAWbtZZw2wZtaKkiTNyG9wS5KaDAtJUpNhIUlqMiwkSU2GhSSpybCQJDUZFpKkJsNCktRkWEiSmgwLSVKTYSFJajIsJElNhoUkqcmwkCQ1GRaSpCbDQpLUZFhIkpoMC0lSk2EhSWoyLCRJTYaFJKnJsJAkNRkWkqQmw0KS1GRYSJKaDAtJUpNhIUlqMiwkSU2GhSSpybCQJDUZFpKkJsNCktRkWEiSmg6YsEhyTpJ7kmxKcum465GkSXJAhEWSecCVwKuAZcD5SZaNtypJmhwHRFgApwObquobVfUUcAOwYsw1SdLEmD/uAoa0CNg8ML8F+JmpnZKsBlb3s08kuWcEtU2CBcBD4y5iLsh7Vo27BD2bf5+7vCP741VeOF3jgRIW070D9ayGqquBq2e/nMmSZGNVLR93HdJ0/PscjQNlGGoLcNLA/GLggTHVIkkT50AJi78FliY5JclhwErg5jHXJEkT44AYhqqqHUneCPwlMA/4cFXdOeayJolDe5rL/PscgVQ9a+hfkqRnOFCGoSRJY2RYSJKaDAvNyNusaK5K8uEk25J8ddy1TALDQrvlbVY0x10LnDPuIiaFYaGZeJsVzVlV9TngkXHXMSkMC81kutusLBpTLZLGyLDQTIa6zYqkg59hoZl4mxVJgGGhmXmbFUmAYaEZVNUOYNdtVu4G1nmbFc0VSa4HvgicmmRLkovGXdPBzNt9SJKaPLOQJDUZFpKkJsNCktRkWEiSmgwLSVKTYSFJajIsJElNhoU0AkneneSSgfk1SX5rnDVJe8Iv5UkjkGQJcGNV/VSSQ4B7gdOr6uGxFiYNaf64C5AmQVX9XZKHk7wUOB74ikGhA4lhIY3ONcDrgRcAHx5vKdKecRhKGpH+zr13AIcCS6tq55hLkobmmYU0IlX1VJLPAI8aFDrQGBbSiPQXts8Azht3LdKe8qOz0ggkWQZsAtZX1b3jrkfaU16zkCQ1eWYhSWoyLCRJTYaFJKnJsJAkNRkWkqSm/w/wQH5D1y3iGgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Count Plot to visualize the counts\n",
    "plt.title('Count Plot of Target Feature')\n",
    "ax = sns.countplot(x=\"y\", data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>x11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.375866</td>\n",
       "      <td>0.427942</td>\n",
       "      <td>-0.922338</td>\n",
       "      <td>0.210758</td>\n",
       "      <td>0.109015</td>\n",
       "      <td>0.621001</td>\n",
       "      <td>-0.444421</td>\n",
       "      <td>0.089970</td>\n",
       "      <td>-0.707711</td>\n",
       "      <td>0.473700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.047819</td>\n",
       "      <td>0.115627</td>\n",
       "      <td>-1.781739</td>\n",
       "      <td>-0.272785</td>\n",
       "      <td>0.392783</td>\n",
       "      <td>1.094168</td>\n",
       "      <td>-0.975254</td>\n",
       "      <td>-0.353424</td>\n",
       "      <td>0.145543</td>\n",
       "      <td>-0.064961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.372868</td>\n",
       "      <td>-0.263291</td>\n",
       "      <td>-1.120545</td>\n",
       "      <td>-0.773828</td>\n",
       "      <td>0.830072</td>\n",
       "      <td>-1.727836</td>\n",
       "      <td>1.323876</td>\n",
       "      <td>-1.587291</td>\n",
       "      <td>-0.024916</td>\n",
       "      <td>0.082491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.059598</td>\n",
       "      <td>0.270797</td>\n",
       "      <td>0.961795</td>\n",
       "      <td>-1.804197</td>\n",
       "      <td>2.931330</td>\n",
       "      <td>1.891656</td>\n",
       "      <td>0.094252</td>\n",
       "      <td>-0.873467</td>\n",
       "      <td>-1.217680</td>\n",
       "      <td>-1.848046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.616319</td>\n",
       "      <td>0.291275</td>\n",
       "      <td>-1.113519</td>\n",
       "      <td>0.626864</td>\n",
       "      <td>-0.287989</td>\n",
       "      <td>-0.842649</td>\n",
       "      <td>-0.947257</td>\n",
       "      <td>1.198215</td>\n",
       "      <td>0.972420</td>\n",
       "      <td>-1.054313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x2        x3        x4        x5        x6        x7        x8  \\\n",
       "0 -0.375866  0.427942 -0.922338  0.210758  0.109015  0.621001 -0.444421   \n",
       "1  0.047819  0.115627 -1.781739 -0.272785  0.392783  1.094168 -0.975254   \n",
       "2  0.372868 -0.263291 -1.120545 -0.773828  0.830072 -1.727836  1.323876   \n",
       "3  0.059598  0.270797  0.961795 -1.804197  2.931330  1.891656  0.094252   \n",
       "4  0.616319  0.291275 -1.113519  0.626864 -0.287989 -0.842649 -0.947257   \n",
       "\n",
       "         x9       x10       x11  \n",
       "0  0.089970 -0.707711  0.473700  \n",
       "1 -0.353424  0.145543 -0.064961  \n",
       "2 -1.587291 -0.024916  0.082491  \n",
       "3 -0.873467 -1.217680 -1.848046  \n",
       "4  1.198215  0.972420 -1.054313  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dropping the original categorical variable\n",
    "df1=df.iloc[:,2:] \n",
    "y=df.iloc[:,0]\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>x11</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.375866</td>\n",
       "      <td>0.427942</td>\n",
       "      <td>-0.922338</td>\n",
       "      <td>0.210758</td>\n",
       "      <td>0.109015</td>\n",
       "      <td>0.621001</td>\n",
       "      <td>-0.444421</td>\n",
       "      <td>0.089970</td>\n",
       "      <td>-0.707711</td>\n",
       "      <td>0.473700</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.047819</td>\n",
       "      <td>0.115627</td>\n",
       "      <td>-1.781739</td>\n",
       "      <td>-0.272785</td>\n",
       "      <td>0.392783</td>\n",
       "      <td>1.094168</td>\n",
       "      <td>-0.975254</td>\n",
       "      <td>-0.353424</td>\n",
       "      <td>0.145543</td>\n",
       "      <td>-0.064961</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.372868</td>\n",
       "      <td>-0.263291</td>\n",
       "      <td>-1.120545</td>\n",
       "      <td>-0.773828</td>\n",
       "      <td>0.830072</td>\n",
       "      <td>-1.727836</td>\n",
       "      <td>1.323876</td>\n",
       "      <td>-1.587291</td>\n",
       "      <td>-0.024916</td>\n",
       "      <td>0.082491</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.059598</td>\n",
       "      <td>0.270797</td>\n",
       "      <td>0.961795</td>\n",
       "      <td>-1.804197</td>\n",
       "      <td>2.931330</td>\n",
       "      <td>1.891656</td>\n",
       "      <td>0.094252</td>\n",
       "      <td>-0.873467</td>\n",
       "      <td>-1.217680</td>\n",
       "      <td>-1.848046</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.616319</td>\n",
       "      <td>0.291275</td>\n",
       "      <td>-1.113519</td>\n",
       "      <td>0.626864</td>\n",
       "      <td>-0.287989</td>\n",
       "      <td>-0.842649</td>\n",
       "      <td>-0.947257</td>\n",
       "      <td>1.198215</td>\n",
       "      <td>0.972420</td>\n",
       "      <td>-1.054313</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x2        x3        x4        x5        x6        x7        x8  \\\n",
       "0 -0.375866  0.427942 -0.922338  0.210758  0.109015  0.621001 -0.444421   \n",
       "1  0.047819  0.115627 -1.781739 -0.272785  0.392783  1.094168 -0.975254   \n",
       "2  0.372868 -0.263291 -1.120545 -0.773828  0.830072 -1.727836  1.323876   \n",
       "3  0.059598  0.270797  0.961795 -1.804197  2.931330  1.891656  0.094252   \n",
       "4  0.616319  0.291275 -1.113519  0.626864 -0.287989 -0.842649 -0.947257   \n",
       "\n",
       "         x9       x10       x11  b  c  d  \n",
       "0  0.089970 -0.707711  0.473700  0  0  0  \n",
       "1 -0.353424  0.145543 -0.064961  1  0  0  \n",
       "2 -1.587291 -0.024916  0.082491  0  0  1  \n",
       "3 -0.873467 -1.217680 -1.848046  0  1  0  \n",
       "4  1.198215  0.972420 -1.054313  0  0  1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#concatenating the one hot encoded columns to the dataframe\n",
    "df_new=pd.concat([df1,df_x1],axis=1)\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>x11</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1700.000000</td>\n",
       "      <td>1700.000000</td>\n",
       "      <td>1700.000000</td>\n",
       "      <td>1700.000000</td>\n",
       "      <td>1700.000000</td>\n",
       "      <td>1700.000000</td>\n",
       "      <td>1700.000000</td>\n",
       "      <td>1700.000000</td>\n",
       "      <td>1700.000000</td>\n",
       "      <td>1700.000000</td>\n",
       "      <td>1700.000000</td>\n",
       "      <td>1700.000000</td>\n",
       "      <td>1700.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.066277</td>\n",
       "      <td>0.025411</td>\n",
       "      <td>-0.004941</td>\n",
       "      <td>-0.018150</td>\n",
       "      <td>0.004139</td>\n",
       "      <td>-0.004442</td>\n",
       "      <td>0.039203</td>\n",
       "      <td>-0.024454</td>\n",
       "      <td>-0.008586</td>\n",
       "      <td>0.005379</td>\n",
       "      <td>0.253529</td>\n",
       "      <td>0.234706</td>\n",
       "      <td>0.258235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.421414</td>\n",
       "      <td>0.284509</td>\n",
       "      <td>1.001383</td>\n",
       "      <td>0.983988</td>\n",
       "      <td>1.031957</td>\n",
       "      <td>1.033610</td>\n",
       "      <td>0.978798</td>\n",
       "      <td>0.987621</td>\n",
       "      <td>0.993979</td>\n",
       "      <td>1.017824</td>\n",
       "      <td>0.435159</td>\n",
       "      <td>0.423940</td>\n",
       "      <td>0.437793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.925228</td>\n",
       "      <td>-0.931249</td>\n",
       "      <td>-3.547109</td>\n",
       "      <td>-4.166713</td>\n",
       "      <td>-3.271321</td>\n",
       "      <td>-3.050503</td>\n",
       "      <td>-3.188723</td>\n",
       "      <td>-3.691485</td>\n",
       "      <td>-3.426298</td>\n",
       "      <td>-4.075024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.383928</td>\n",
       "      <td>-0.189961</td>\n",
       "      <td>-0.693761</td>\n",
       "      <td>-0.630827</td>\n",
       "      <td>-0.717443</td>\n",
       "      <td>-0.679205</td>\n",
       "      <td>-0.606272</td>\n",
       "      <td>-0.691628</td>\n",
       "      <td>-0.643558</td>\n",
       "      <td>-0.679580</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.018826</td>\n",
       "      <td>0.011861</td>\n",
       "      <td>0.002496</td>\n",
       "      <td>0.008173</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>-0.021232</td>\n",
       "      <td>0.049557</td>\n",
       "      <td>-0.040668</td>\n",
       "      <td>0.012354</td>\n",
       "      <td>0.009655</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.156138</td>\n",
       "      <td>0.270204</td>\n",
       "      <td>0.697783</td>\n",
       "      <td>0.638092</td>\n",
       "      <td>0.748816</td>\n",
       "      <td>0.713769</td>\n",
       "      <td>0.673363</td>\n",
       "      <td>0.629988</td>\n",
       "      <td>0.654637</td>\n",
       "      <td>0.692432</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.696405</td>\n",
       "      <td>0.567431</td>\n",
       "      <td>2.977407</td>\n",
       "      <td>3.434698</td>\n",
       "      <td>3.575212</td>\n",
       "      <td>3.387439</td>\n",
       "      <td>4.651006</td>\n",
       "      <td>3.763927</td>\n",
       "      <td>3.259833</td>\n",
       "      <td>3.095674</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                x2           x3           x4           x5           x6  \\\n",
       "count  1700.000000  1700.000000  1700.000000  1700.000000  1700.000000   \n",
       "mean     -0.066277     0.025411    -0.004941    -0.018150     0.004139   \n",
       "std       0.421414     0.284509     1.001383     0.983988     1.031957   \n",
       "min      -0.925228    -0.931249    -3.547109    -4.166713    -3.271321   \n",
       "25%      -0.383928    -0.189961    -0.693761    -0.630827    -0.717443   \n",
       "50%       0.018826     0.011861     0.002496     0.008173     0.015596   \n",
       "75%       0.156138     0.270204     0.697783     0.638092     0.748816   \n",
       "max       0.696405     0.567431     2.977407     3.434698     3.575212   \n",
       "\n",
       "                x7           x8           x9          x10          x11  \\\n",
       "count  1700.000000  1700.000000  1700.000000  1700.000000  1700.000000   \n",
       "mean     -0.004442     0.039203    -0.024454    -0.008586     0.005379   \n",
       "std       1.033610     0.978798     0.987621     0.993979     1.017824   \n",
       "min      -3.050503    -3.188723    -3.691485    -3.426298    -4.075024   \n",
       "25%      -0.679205    -0.606272    -0.691628    -0.643558    -0.679580   \n",
       "50%      -0.021232     0.049557    -0.040668     0.012354     0.009655   \n",
       "75%       0.713769     0.673363     0.629988     0.654637     0.692432   \n",
       "max       3.387439     4.651006     3.763927     3.259833     3.095674   \n",
       "\n",
       "                 b            c            d  \n",
       "count  1700.000000  1700.000000  1700.000000  \n",
       "mean      0.253529     0.234706     0.258235  \n",
       "std       0.435159     0.423940     0.437793  \n",
       "min       0.000000     0.000000     0.000000  \n",
       "25%       0.000000     0.000000     0.000000  \n",
       "50%       0.000000     0.000000     0.000000  \n",
       "75%       1.000000     0.000000     1.000000  \n",
       "max       1.000000     1.000000     1.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.describe(include='all') #statistics of each column/feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x2     0\n",
       "x3     0\n",
       "x4     0\n",
       "x5     0\n",
       "x6     0\n",
       "x7     0\n",
       "x8     0\n",
       "x9     0\n",
       "x10    0\n",
       "x11    0\n",
       "b      0\n",
       "c      0\n",
       "d      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the total number of null values in each column/feature\n",
    "df_new.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1275, 13) (425, 13) (1275,) (425,)\n"
     ]
    }
   ],
   "source": [
    "#Splitting the data to train and test\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "df_new.values, y, random_state=0)\n",
    "print(x_train.shape,x_test.shape,y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Model, Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5317647058823529"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Building a dummy baseline model. This model just assigns the majority class as predicted target class for all records.\n",
    "dummy = DummyClassifier(strategy='stratified', random_state=1)\n",
    "dummy.fit(x_train,y_train)\n",
    "dummy.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize the features, so that no feature dominates, and so that the scales can be ignored\n",
    "scaler= StandardScaler()\n",
    "x_standardized = scaler.fit_transform(x_train)\n",
    "x_test_st=scaler.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#I used logistic regression as it can be interpreted easily as opposed to algorithms like randomforests. If we want to know \n",
    "#why certain records were misclassified we can look at the feature coefficients, and see how/why the model wrongly classified that data point\n",
    "logistic_regression = LogisticRegression(random_state=0)\n",
    "lr_model=logistic_regression.fit(x_standardized,y_train) #fitting the model\n",
    "y_pred=lr_model.predict(x_test_st) #predicting values for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.91      0.90       256\n",
      "           1       0.86      0.82      0.84       169\n",
      "\n",
      "    accuracy                           0.88       425\n",
      "   macro avg       0.87      0.87      0.87       425\n",
      "weighted avg       0.88      0.88      0.88       425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))\n",
    "#precision = TP/TP+FP\n",
    "#recall = TP/TP+FN\n",
    "#F1 score = 2*Precision*Recall/(Precision+Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.97343071, -3.11845574,  0.05190365,  0.03230276, -0.06920039,\n",
       "        -0.07665628, -0.06031583,  0.11963035,  0.21816348, -0.01492455,\n",
       "         0.01277306, -0.0838549 , -0.01587319]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The coefficients of the features\n",
    "lr_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression with a list of Cs(1/regularization strength parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#we do a cross validated logistic regression. In this case we are trying to find the best C(regularization strength parameter). \n",
    "lr_cv = LogisticRegressionCV(\n",
    " penalty='l1', Cs=10, random_state=0,solver='liblinear')\n",
    "lr_cv = lr_cv.fit(x_standardized, y_train)\n",
    "y_pred_cv=lr_cv.predict(x_test_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.35938137])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The chosen C\n",
    "lr_cv.C_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.91      0.90       256\n",
      "           1       0.86      0.82      0.84       169\n",
      "\n",
      "    accuracy                           0.88       425\n",
      "   macro avg       0.87      0.87      0.87       425\n",
      "weighted avg       0.87      0.88      0.87       425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression with Gridsearch to find the best combination of parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Performing Gridsearch to find the best combination of hyperparameters(in this case what type of regularization and C)\n",
    "penalties = ['l1', 'l2'] #penalty list\n",
    "Cs = np.logspace(0, 15, 10) #range of Cs\n",
    "hyperparameters = dict(C=Cs, penalty=penalties) #dictionary of hyperparameters\n",
    "gridsearch = GridSearchCV(logistic_regression, hyperparameters, cv=7, verbose=0) #gridsearch to find the best combination of parameters\n",
    "b_m = gridsearch.fit(x_standardized, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'l1'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_m.best_estimator_.get_params()['penalty'] #penalty chosen by the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_m.best_estimator_.get_params()['C'] #regularization strength chosen by best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.91      0.90       256\n",
      "           1       0.86      0.83      0.84       169\n",
      "\n",
      "    accuracy                           0.88       425\n",
      "   macro avg       0.87      0.87      0.87       425\n",
      "weighted avg       0.88      0.88      0.88       425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bm_y_pred=b_m.predict(x_test_st)\n",
    "print(classification_report(y_test, bm_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding F1 Scores for different threshold values, to check which threshold value is best for the Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_li = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99] #list of thresholds\n",
    "pred_proba_df = pd.DataFrame(b_m.predict_proba(x_test_st)) #dataframe of class probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.656705</td>\n",
       "      <td>0.343295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.979927</td>\n",
       "      <td>0.020073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.686832</td>\n",
       "      <td>0.313168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.999939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1\n",
       "0  0.656705  0.343295\n",
       "1  0.979927  0.020073\n",
       "2  0.999988  0.000012\n",
       "3  0.686832  0.313168\n",
       "4  0.000061  0.999939"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_proba_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******** For i = 0.05 ******\n",
      "f1_score is 0.786046511627907\n",
      "\n",
      "******** For i = 0.1 ******\n",
      "f1_score is 0.8066825775656324\n",
      "\n",
      "******** For i = 0.15 ******\n",
      "f1_score is 0.8144578313253013\n",
      "\n",
      "******** For i = 0.2 ******\n",
      "f1_score is 0.824390243902439\n",
      "\n",
      "******** For i = 0.25 ******\n",
      "f1_score is 0.8279301745635911\n",
      "\n",
      "******** For i = 0.3 ******\n",
      "f1_score is 0.8186528497409326\n",
      "\n",
      "******** For i = 0.35 ******\n",
      "f1_score is 0.8086253369272238\n",
      "\n",
      "******** For i = 0.4 ******\n",
      "f1_score is 0.8112676056338027\n",
      "\n",
      "******** For i = 0.45 ******\n",
      "f1_score is 0.8187134502923976\n",
      "\n",
      "******** For i = 0.5 ******\n",
      "f1_score is 0.8433734939759036\n",
      "\n",
      "******** For i = 0.55 ******\n",
      "f1_score is 0.839506172839506\n",
      "\n",
      "******** For i = 0.6 ******\n",
      "f1_score is 0.8417721518987341\n",
      "\n",
      "******** For i = 0.65 ******\n",
      "f1_score is 0.8403908794788274\n",
      "\n",
      "******** For i = 0.7 ******\n",
      "f1_score is 0.8305647840531561\n",
      "\n",
      "******** For i = 0.75 ******\n",
      "f1_score is 0.8175675675675675\n",
      "\n",
      "******** For i = 0.8 ******\n",
      "f1_score is 0.8096885813148789\n",
      "\n",
      "******** For i = 0.85 ******\n",
      "f1_score is 0.8041958041958042\n",
      "\n",
      "******** For i = 0.9 ******\n",
      "f1_score is 0.7872340425531915\n",
      "\n",
      "******** For i = 0.95 ******\n",
      "f1_score is 0.7563636363636365\n",
      "\n",
      "******** For i = 0.99 ******\n",
      "f1_score is 0.6507936507936508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Finding the F1 scores for different threshold values to check which threshoold gives the best F1 score\n",
    "f1_list=[]\n",
    "for i in th_li:\n",
    "    print ('\\n******** For i = {} ******'.format(i))\n",
    "    y_test_pred = pred_proba_df.applymap(lambda x: 1 if x>i else 0) #make the value 1 if the value more than a particular threshold\n",
    "    f1 =f1_score(y_test.as_matrix().reshape(y_test.as_matrix().size,1),y_test_pred.iloc[:,1].as_matrix().reshape(y_test_pred.iloc[:,1].as_matrix().size,1))\n",
    "    print('f1_score is {}'.format(f1))\n",
    "    f1_list.append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('F1 SCORE VS THRESHOLDS')\n",
    "plt.xlabel('Thresholds')\n",
    "plt.ylabel('F1 Scores')\n",
    "plt.plot(th_li,f1_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       256\n",
      "           1       0.99      0.98      0.99       169\n",
      "\n",
      "    accuracy                           0.99       425\n",
      "   macro avg       0.99      0.99      0.99       425\n",
      "weighted avg       0.99      0.99      0.99       425\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=0)\n",
    "rf.fit(x_standardized, y_train)\n",
    "rf_y_pred=rf.predict(x_test_st)\n",
    "print(classification_report(y_test, rf_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding Importances of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The importances are: [0.49507323 0.34918409 0.01713779 0.0233304  0.0221065  0.01322352\n",
      " 0.02012334 0.01539666 0.02195485 0.01735592 0.00089685 0.00266862\n",
      " 0.00154822]\n",
      "The importance of features in decreasing order are: [ 0  1  3  4  8  6  9  2  7  5 11 12 10]\n"
     ]
    }
   ],
   "source": [
    "#Looking at the feature importances we see that the first two features are the most important(x2 and x3).\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]#sorting the indices by importance\n",
    "print(\"The importances are:\",importances)\n",
    "print(\"The importance of features in decreasing order are:\",indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Random Forest Model with five of the most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1275, 2)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_indices = [0,1]\n",
    "x_train_imp=np.take(x_standardized, filter_indices, axis=1) #keeping only top 2 features with highest importance\n",
    "x_train_imp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(425, 2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_imp=np.take(x_test_st, filter_indices, axis=1)\n",
    "x_test_imp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       256\n",
      "           1       0.99      0.98      0.99       169\n",
      "\n",
      "    accuracy                           0.99       425\n",
      "   macro avg       0.99      0.99      0.99       425\n",
      "weighted avg       0.99      0.99      0.99       425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Building a random forest model with only the first two features we see that the performance is as good as the performance of\n",
    "# the random forest with all the features considered\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "rf.fit(x_train_imp, y_train)\n",
    "rf_y_pred=rf.predict(x_test_imp)\n",
    "print(classification_report(y_test, rf_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Imbalanced Class with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       256\n",
      "           1       0.99      0.98      0.99       169\n",
      "\n",
      "    accuracy                           0.99       425\n",
      "   macro avg       0.99      0.99      0.99       425\n",
      "weighted avg       0.99      0.99      0.99       425\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# As the classes are imbalanced I built a random forest model with balanced class weight. What this does this give a higher weight\n",
    "# to minority class than the majority class. We see that the performance after weighting is excellent.\n",
    "rf = RandomForestClassifier(\n",
    " random_state=0, class_weight=\"balanced\")\n",
    "# class weight = n/(k * nj)\n",
    "#n=total number of observations, k= total number of classes, nj=number of observations in class j\n",
    "rf = rf.fit(x_standardized, y_train)\n",
    "rf_bal_pred=rf.predict(x_test_st)\n",
    "print(classification_report(y_test, rf_bal_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaboost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       256\n",
      "           1       1.00      0.98      0.99       169\n",
      "\n",
      "    accuracy                           0.99       425\n",
      "   macro avg       0.99      0.99      0.99       425\n",
      "weighted avg       0.99      0.99      0.99       425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Boosting algorithm performance on the dataset.\n",
    "adaboost = AdaBoostClassifier(random_state=0)\n",
    "ad = adaboost.fit(x_standardized, y_train)\n",
    "ad_pred=ad.predict(x_test_st)\n",
    "print(classification_report(y_test, ad_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding Records that were misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  8,  14,  15,  20,  35,  38,  43,  52,  57,  60,  63,  64,  70,\n",
       "         72,  73,  79,  86,  94,  97, 102, 112, 122, 124, 126, 130, 139,\n",
       "        154, 174, 176, 181, 182, 189, 208, 209, 242, 269, 293, 303, 305,\n",
       "        321, 328, 336, 348, 360, 363, 364, 370, 371, 376, 382, 407, 413,\n",
       "        422], dtype=int64),)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#indices of all records that were misclassified, using logistiregressioncv model\n",
    "mis = np.where(y_test != lr_cv.predict(x_test_st))\n",
    "mis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.31299201, -3.30191002,  0.0112637 ,  0.        , -0.03151588,\n",
       "        -0.04471731, -0.02620369,  0.09576702,  0.18453021,  0.        ,\n",
       "         0.        , -0.05470971,  0.        ]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_cv.coef_ #The coeffiecients of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_test=pd.concat([pd.DataFrame(x_test_st),y_test,pd.Series(y_pred_cv)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(425, 13)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_st.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(425, 1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(y_test, axis=1).shape #to provide second dimension to ytest, same procedure is done for ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test=np.hstack((x_test_st,np.expand_dims(y_test, axis=1),np.expand_dims(y_pred_cv, axis=1))) #concatenating the data, actual targets, predicted vallues together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.509487</td>\n",
       "      <td>0.486423</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.552058</td>\n",
       "      <td>-0.513146</td>\n",
       "      <td>-0.050364</td>\n",
       "      <td>-1.724537</td>\n",
       "      <td>-0.036223</td>\n",
       "      <td>0.633430</td>\n",
       "      <td>0.567305</td>\n",
       "      <td>-0.62361</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>1.779513</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.529733</td>\n",
       "      <td>1.110619</td>\n",
       "      <td>-1.388080</td>\n",
       "      <td>-0.556127</td>\n",
       "      <td>0.718453</td>\n",
       "      <td>0.826472</td>\n",
       "      <td>-0.942145</td>\n",
       "      <td>-0.502333</td>\n",
       "      <td>-1.746293</td>\n",
       "      <td>-1.101316</td>\n",
       "      <td>-0.62361</td>\n",
       "      <td>1.713238</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.531939</td>\n",
       "      <td>1.518358</td>\n",
       "      <td>-1.766048</td>\n",
       "      <td>0.631121</td>\n",
       "      <td>-0.044027</td>\n",
       "      <td>-0.658872</td>\n",
       "      <td>1.115515</td>\n",
       "      <td>0.452673</td>\n",
       "      <td>-0.963870</td>\n",
       "      <td>-0.483150</td>\n",
       "      <td>-0.62361</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>1.779513</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.228240</td>\n",
       "      <td>-0.075905</td>\n",
       "      <td>1.848626</td>\n",
       "      <td>-0.334706</td>\n",
       "      <td>1.488867</td>\n",
       "      <td>0.571497</td>\n",
       "      <td>0.401286</td>\n",
       "      <td>-0.415626</td>\n",
       "      <td>-0.602191</td>\n",
       "      <td>0.773366</td>\n",
       "      <td>-0.62361</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>1.779513</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.611387</td>\n",
       "      <td>-0.484740</td>\n",
       "      <td>1.211679</td>\n",
       "      <td>-2.936406</td>\n",
       "      <td>-0.043080</td>\n",
       "      <td>-0.044310</td>\n",
       "      <td>0.414860</td>\n",
       "      <td>0.539381</td>\n",
       "      <td>1.112503</td>\n",
       "      <td>-0.632662</td>\n",
       "      <td>-0.62361</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>1.779513</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.509487  0.486423  0.018868  0.552058 -0.513146 -0.050364 -1.724537   \n",
       "1  0.529733  1.110619 -1.388080 -0.556127  0.718453  0.826472 -0.942145   \n",
       "2 -0.531939  1.518358 -1.766048  0.631121 -0.044027 -0.658872  1.115515   \n",
       "3  0.228240 -0.075905  1.848626 -0.334706  1.488867  0.571497  0.401286   \n",
       "4  1.611387 -0.484740  1.211679 -2.936406 -0.043080 -0.044310  0.414860   \n",
       "\n",
       "         7         8         9        10        11        12   13   14  \n",
       "0 -0.036223  0.633430  0.567305 -0.62361 -0.583690  1.779513  0.0  0.0  \n",
       "1 -0.502333 -1.746293 -1.101316 -0.62361  1.713238 -0.561951  0.0  0.0  \n",
       "2  0.452673 -0.963870 -0.483150 -0.62361 -0.583690  1.779513  0.0  0.0  \n",
       "3 -0.415626 -0.602191  0.773366 -0.62361 -0.583690  1.779513  0.0  0.0  \n",
       "4  0.539381  1.112503 -0.632662 -0.62361 -0.583690  1.779513  1.0  1.0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test=pd.DataFrame(data_test)\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.31299201, -3.30191002,  0.0112637 ,  0.        , -0.03151588,\n",
       "        -0.04471731, -0.02620369,  0.09576702,  0.18453021,  0.        ,\n",
       "         0.        , -0.05470971,  0.        ]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_cv.coef_ #coefficients of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.570477</td>\n",
       "      <td>0.213404</td>\n",
       "      <td>0.890349</td>\n",
       "      <td>-0.147439</td>\n",
       "      <td>1.990126</td>\n",
       "      <td>-0.761451</td>\n",
       "      <td>0.339955</td>\n",
       "      <td>-0.810964</td>\n",
       "      <td>1.438997</td>\n",
       "      <td>0.287414</td>\n",
       "      <td>1.603567</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.124164</td>\n",
       "      <td>-0.261967</td>\n",
       "      <td>0.898710</td>\n",
       "      <td>-0.212416</td>\n",
       "      <td>0.439933</td>\n",
       "      <td>0.477081</td>\n",
       "      <td>-0.737784</td>\n",
       "      <td>1.681344</td>\n",
       "      <td>-0.114237</td>\n",
       "      <td>1.085141</td>\n",
       "      <td>-0.623610</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>1.779513</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.069134</td>\n",
       "      <td>-0.321564</td>\n",
       "      <td>-0.726354</td>\n",
       "      <td>0.019048</td>\n",
       "      <td>-0.144728</td>\n",
       "      <td>2.590595</td>\n",
       "      <td>0.276171</td>\n",
       "      <td>-1.268726</td>\n",
       "      <td>0.674310</td>\n",
       "      <td>-0.155300</td>\n",
       "      <td>-0.623610</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>1.779513</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.177450</td>\n",
       "      <td>-0.111869</td>\n",
       "      <td>1.260612</td>\n",
       "      <td>-0.337727</td>\n",
       "      <td>0.203715</td>\n",
       "      <td>-1.056401</td>\n",
       "      <td>0.252330</td>\n",
       "      <td>-1.398638</td>\n",
       "      <td>-0.140565</td>\n",
       "      <td>-0.049648</td>\n",
       "      <td>1.603567</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.138507</td>\n",
       "      <td>-0.134612</td>\n",
       "      <td>0.854455</td>\n",
       "      <td>0.364880</td>\n",
       "      <td>1.131730</td>\n",
       "      <td>-1.013236</td>\n",
       "      <td>0.452629</td>\n",
       "      <td>1.127784</td>\n",
       "      <td>-1.360652</td>\n",
       "      <td>-0.704170</td>\n",
       "      <td>1.603567</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.120644</td>\n",
       "      <td>-0.179484</td>\n",
       "      <td>0.656598</td>\n",
       "      <td>0.069878</td>\n",
       "      <td>0.592357</td>\n",
       "      <td>0.282234</td>\n",
       "      <td>-0.945014</td>\n",
       "      <td>-1.551942</td>\n",
       "      <td>-1.379001</td>\n",
       "      <td>-3.006900</td>\n",
       "      <td>1.603567</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.141096</td>\n",
       "      <td>-0.245467</td>\n",
       "      <td>-0.338976</td>\n",
       "      <td>0.503314</td>\n",
       "      <td>-0.816049</td>\n",
       "      <td>-0.633711</td>\n",
       "      <td>0.262273</td>\n",
       "      <td>-2.160652</td>\n",
       "      <td>-0.125377</td>\n",
       "      <td>-1.194919</td>\n",
       "      <td>-0.623610</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.129750</td>\n",
       "      <td>-0.185876</td>\n",
       "      <td>-1.550975</td>\n",
       "      <td>-0.768215</td>\n",
       "      <td>-0.131383</td>\n",
       "      <td>1.256000</td>\n",
       "      <td>1.118221</td>\n",
       "      <td>-0.086961</td>\n",
       "      <td>-0.370840</td>\n",
       "      <td>-1.954281</td>\n",
       "      <td>1.603567</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.054896</td>\n",
       "      <td>-0.214768</td>\n",
       "      <td>-0.227539</td>\n",
       "      <td>-0.186961</td>\n",
       "      <td>-0.961218</td>\n",
       "      <td>1.451368</td>\n",
       "      <td>-0.842000</td>\n",
       "      <td>-0.189125</td>\n",
       "      <td>-0.707262</td>\n",
       "      <td>-1.409180</td>\n",
       "      <td>1.603567</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.429239</td>\n",
       "      <td>0.050459</td>\n",
       "      <td>-1.135358</td>\n",
       "      <td>1.197269</td>\n",
       "      <td>1.182529</td>\n",
       "      <td>-0.965997</td>\n",
       "      <td>-0.730104</td>\n",
       "      <td>-0.844970</td>\n",
       "      <td>1.649886</td>\n",
       "      <td>1.026833</td>\n",
       "      <td>-0.623610</td>\n",
       "      <td>1.713238</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.313153</td>\n",
       "      <td>-0.054648</td>\n",
       "      <td>-1.074729</td>\n",
       "      <td>0.303291</td>\n",
       "      <td>-0.701241</td>\n",
       "      <td>0.889702</td>\n",
       "      <td>1.193255</td>\n",
       "      <td>0.640984</td>\n",
       "      <td>0.888495</td>\n",
       "      <td>-0.468003</td>\n",
       "      <td>-0.623610</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.116480</td>\n",
       "      <td>-0.248494</td>\n",
       "      <td>0.325751</td>\n",
       "      <td>0.116851</td>\n",
       "      <td>0.352193</td>\n",
       "      <td>0.321785</td>\n",
       "      <td>-0.090461</td>\n",
       "      <td>1.615395</td>\n",
       "      <td>-0.562830</td>\n",
       "      <td>-0.808936</td>\n",
       "      <td>-0.623610</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.166282</td>\n",
       "      <td>-0.295967</td>\n",
       "      <td>-0.477405</td>\n",
       "      <td>1.224248</td>\n",
       "      <td>-2.444276</td>\n",
       "      <td>-1.277340</td>\n",
       "      <td>-0.477845</td>\n",
       "      <td>0.347005</td>\n",
       "      <td>-1.055721</td>\n",
       "      <td>-0.323697</td>\n",
       "      <td>1.603567</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>-1.184669</td>\n",
       "      <td>-2.606315</td>\n",
       "      <td>-0.798833</td>\n",
       "      <td>-1.509623</td>\n",
       "      <td>0.663433</td>\n",
       "      <td>-0.166495</td>\n",
       "      <td>-1.843152</td>\n",
       "      <td>0.076270</td>\n",
       "      <td>0.413933</td>\n",
       "      <td>0.227159</td>\n",
       "      <td>-0.623610</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>1.779513</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.405787</td>\n",
       "      <td>-0.035257</td>\n",
       "      <td>0.907566</td>\n",
       "      <td>1.506355</td>\n",
       "      <td>-1.578018</td>\n",
       "      <td>0.780546</td>\n",
       "      <td>-0.270929</td>\n",
       "      <td>-0.879325</td>\n",
       "      <td>1.495713</td>\n",
       "      <td>-0.150713</td>\n",
       "      <td>1.603567</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>-0.916263</td>\n",
       "      <td>-3.188031</td>\n",
       "      <td>-2.000470</td>\n",
       "      <td>0.187104</td>\n",
       "      <td>-0.624058</td>\n",
       "      <td>-0.773404</td>\n",
       "      <td>1.014346</td>\n",
       "      <td>0.219134</td>\n",
       "      <td>-0.782451</td>\n",
       "      <td>-0.894581</td>\n",
       "      <td>-0.623610</td>\n",
       "      <td>1.713238</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.533947</td>\n",
       "      <td>0.017610</td>\n",
       "      <td>1.321926</td>\n",
       "      <td>-0.344979</td>\n",
       "      <td>-1.306716</td>\n",
       "      <td>-0.991680</td>\n",
       "      <td>0.303015</td>\n",
       "      <td>-1.274544</td>\n",
       "      <td>0.989536</td>\n",
       "      <td>-2.138604</td>\n",
       "      <td>-0.623610</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>1.779513</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.057157</td>\n",
       "      <td>-0.482965</td>\n",
       "      <td>-2.347794</td>\n",
       "      <td>-0.215441</td>\n",
       "      <td>0.818455</td>\n",
       "      <td>0.080684</td>\n",
       "      <td>0.232333</td>\n",
       "      <td>0.854870</td>\n",
       "      <td>-0.921479</td>\n",
       "      <td>-0.120332</td>\n",
       "      <td>-0.623610</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>1.779513</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.176835</td>\n",
       "      <td>-0.100631</td>\n",
       "      <td>-0.094519</td>\n",
       "      <td>0.916606</td>\n",
       "      <td>2.218899</td>\n",
       "      <td>-0.904311</td>\n",
       "      <td>-0.927912</td>\n",
       "      <td>-0.344575</td>\n",
       "      <td>-0.115748</td>\n",
       "      <td>-0.965014</td>\n",
       "      <td>-0.623610</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>1.779513</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.113415</td>\n",
       "      <td>-0.281718</td>\n",
       "      <td>-0.519737</td>\n",
       "      <td>-1.488765</td>\n",
       "      <td>0.642339</td>\n",
       "      <td>0.944480</td>\n",
       "      <td>1.146044</td>\n",
       "      <td>-1.430010</td>\n",
       "      <td>0.607518</td>\n",
       "      <td>0.045193</td>\n",
       "      <td>-0.623610</td>\n",
       "      <td>1.713238</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.098399</td>\n",
       "      <td>-0.329514</td>\n",
       "      <td>0.721658</td>\n",
       "      <td>0.597793</td>\n",
       "      <td>-2.633525</td>\n",
       "      <td>-0.702436</td>\n",
       "      <td>0.251464</td>\n",
       "      <td>-0.450606</td>\n",
       "      <td>-2.243365</td>\n",
       "      <td>0.124079</td>\n",
       "      <td>-0.623610</td>\n",
       "      <td>1.713238</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.173240</td>\n",
       "      <td>-0.105694</td>\n",
       "      <td>0.441679</td>\n",
       "      <td>0.058440</td>\n",
       "      <td>-1.951499</td>\n",
       "      <td>-0.203288</td>\n",
       "      <td>0.405258</td>\n",
       "      <td>-0.669158</td>\n",
       "      <td>-0.593083</td>\n",
       "      <td>-1.728092</td>\n",
       "      <td>-0.623610</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.364203</td>\n",
       "      <td>0.049917</td>\n",
       "      <td>-0.197488</td>\n",
       "      <td>-0.783216</td>\n",
       "      <td>-1.426864</td>\n",
       "      <td>0.319758</td>\n",
       "      <td>-0.119460</td>\n",
       "      <td>0.897862</td>\n",
       "      <td>0.598068</td>\n",
       "      <td>-0.779030</td>\n",
       "      <td>-0.623610</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.150924</td>\n",
       "      <td>-0.142967</td>\n",
       "      <td>0.384473</td>\n",
       "      <td>-0.672574</td>\n",
       "      <td>-1.060597</td>\n",
       "      <td>-0.517821</td>\n",
       "      <td>1.546843</td>\n",
       "      <td>-0.803415</td>\n",
       "      <td>-0.049176</td>\n",
       "      <td>-0.272014</td>\n",
       "      <td>-0.623610</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>1.779513</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.106728</td>\n",
       "      <td>-0.264637</td>\n",
       "      <td>-0.932090</td>\n",
       "      <td>-2.582287</td>\n",
       "      <td>0.344004</td>\n",
       "      <td>0.065492</td>\n",
       "      <td>0.327097</td>\n",
       "      <td>0.181750</td>\n",
       "      <td>1.450440</td>\n",
       "      <td>-1.140263</td>\n",
       "      <td>-0.623610</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.126743</td>\n",
       "      <td>-0.370473</td>\n",
       "      <td>-0.999558</td>\n",
       "      <td>0.210221</td>\n",
       "      <td>-0.471062</td>\n",
       "      <td>-0.270545</td>\n",
       "      <td>-1.104174</td>\n",
       "      <td>-1.398525</td>\n",
       "      <td>0.091643</td>\n",
       "      <td>-0.710227</td>\n",
       "      <td>-0.623610</td>\n",
       "      <td>1.713238</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0.468681</td>\n",
       "      <td>0.050437</td>\n",
       "      <td>-1.355537</td>\n",
       "      <td>0.811363</td>\n",
       "      <td>-1.344335</td>\n",
       "      <td>0.449845</td>\n",
       "      <td>-0.908124</td>\n",
       "      <td>0.483980</td>\n",
       "      <td>1.713257</td>\n",
       "      <td>0.983963</td>\n",
       "      <td>-0.623610</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>-1.222457</td>\n",
       "      <td>-2.835874</td>\n",
       "      <td>0.673598</td>\n",
       "      <td>1.476609</td>\n",
       "      <td>-0.711814</td>\n",
       "      <td>1.176995</td>\n",
       "      <td>0.549289</td>\n",
       "      <td>1.215586</td>\n",
       "      <td>0.006918</td>\n",
       "      <td>0.544925</td>\n",
       "      <td>-0.623610</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0.098747</td>\n",
       "      <td>-0.302331</td>\n",
       "      <td>-0.063855</td>\n",
       "      <td>1.978516</td>\n",
       "      <td>0.195140</td>\n",
       "      <td>-0.543343</td>\n",
       "      <td>0.433188</td>\n",
       "      <td>0.044345</td>\n",
       "      <td>-0.088830</td>\n",
       "      <td>1.739541</td>\n",
       "      <td>-0.623610</td>\n",
       "      <td>1.713238</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>0.543454</td>\n",
       "      <td>0.062956</td>\n",
       "      <td>-0.896043</td>\n",
       "      <td>2.059291</td>\n",
       "      <td>-0.529956</td>\n",
       "      <td>0.216319</td>\n",
       "      <td>-0.551943</td>\n",
       "      <td>-2.190064</td>\n",
       "      <td>0.619470</td>\n",
       "      <td>0.036925</td>\n",
       "      <td>-0.623610</td>\n",
       "      <td>1.713238</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>0.097527</td>\n",
       "      <td>-0.243130</td>\n",
       "      <td>0.441559</td>\n",
       "      <td>0.351539</td>\n",
       "      <td>-0.445331</td>\n",
       "      <td>-0.650953</td>\n",
       "      <td>-0.734543</td>\n",
       "      <td>1.715815</td>\n",
       "      <td>0.444283</td>\n",
       "      <td>-0.591329</td>\n",
       "      <td>-0.623610</td>\n",
       "      <td>1.713238</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0.636393</td>\n",
       "      <td>0.365879</td>\n",
       "      <td>-0.143797</td>\n",
       "      <td>-0.246778</td>\n",
       "      <td>1.655661</td>\n",
       "      <td>-1.857235</td>\n",
       "      <td>1.343687</td>\n",
       "      <td>0.394545</td>\n",
       "      <td>-0.504834</td>\n",
       "      <td>0.631005</td>\n",
       "      <td>-0.623610</td>\n",
       "      <td>1.713238</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>0.116067</td>\n",
       "      <td>-0.201666</td>\n",
       "      <td>0.334041</td>\n",
       "      <td>0.555705</td>\n",
       "      <td>-0.878623</td>\n",
       "      <td>-0.042333</td>\n",
       "      <td>0.399595</td>\n",
       "      <td>0.338007</td>\n",
       "      <td>0.376224</td>\n",
       "      <td>-0.751949</td>\n",
       "      <td>1.603567</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>0.426195</td>\n",
       "      <td>0.077380</td>\n",
       "      <td>-0.952572</td>\n",
       "      <td>-0.055420</td>\n",
       "      <td>-0.948238</td>\n",
       "      <td>0.365358</td>\n",
       "      <td>-0.910338</td>\n",
       "      <td>0.739330</td>\n",
       "      <td>-0.072793</td>\n",
       "      <td>0.490103</td>\n",
       "      <td>1.603567</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>0.107171</td>\n",
       "      <td>-0.292872</td>\n",
       "      <td>-0.603940</td>\n",
       "      <td>0.536668</td>\n",
       "      <td>-0.075732</td>\n",
       "      <td>1.787608</td>\n",
       "      <td>-0.191817</td>\n",
       "      <td>-1.507226</td>\n",
       "      <td>0.606685</td>\n",
       "      <td>0.319507</td>\n",
       "      <td>-0.623610</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>1.779513</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>0.132037</td>\n",
       "      <td>-0.151017</td>\n",
       "      <td>-0.863842</td>\n",
       "      <td>-2.132282</td>\n",
       "      <td>0.727950</td>\n",
       "      <td>1.337556</td>\n",
       "      <td>-0.696062</td>\n",
       "      <td>0.359005</td>\n",
       "      <td>-0.173122</td>\n",
       "      <td>-0.217989</td>\n",
       "      <td>-0.623610</td>\n",
       "      <td>1.713238</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>0.460032</td>\n",
       "      <td>0.072800</td>\n",
       "      <td>0.597998</td>\n",
       "      <td>0.130124</td>\n",
       "      <td>-0.344579</td>\n",
       "      <td>0.103229</td>\n",
       "      <td>1.487559</td>\n",
       "      <td>1.079272</td>\n",
       "      <td>0.513538</td>\n",
       "      <td>0.041187</td>\n",
       "      <td>-0.623610</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>0.335006</td>\n",
       "      <td>-0.076516</td>\n",
       "      <td>-0.228675</td>\n",
       "      <td>0.005427</td>\n",
       "      <td>1.023121</td>\n",
       "      <td>1.418721</td>\n",
       "      <td>-0.596386</td>\n",
       "      <td>-0.101274</td>\n",
       "      <td>1.036712</td>\n",
       "      <td>0.625963</td>\n",
       "      <td>-0.623610</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>1.779513</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>-0.674799</td>\n",
       "      <td>-3.314446</td>\n",
       "      <td>1.569892</td>\n",
       "      <td>-1.165726</td>\n",
       "      <td>0.511096</td>\n",
       "      <td>0.427927</td>\n",
       "      <td>1.421894</td>\n",
       "      <td>-0.085293</td>\n",
       "      <td>-0.461342</td>\n",
       "      <td>-0.234540</td>\n",
       "      <td>1.603567</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>-1.177411</td>\n",
       "      <td>-2.842691</td>\n",
       "      <td>-0.817647</td>\n",
       "      <td>0.420797</td>\n",
       "      <td>-0.436232</td>\n",
       "      <td>0.764009</td>\n",
       "      <td>0.922755</td>\n",
       "      <td>-0.511361</td>\n",
       "      <td>0.195164</td>\n",
       "      <td>-0.895794</td>\n",
       "      <td>-0.623610</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>1.779513</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>0.139323</td>\n",
       "      <td>-0.169897</td>\n",
       "      <td>0.736484</td>\n",
       "      <td>-0.771523</td>\n",
       "      <td>0.343673</td>\n",
       "      <td>-0.376998</td>\n",
       "      <td>-0.565768</td>\n",
       "      <td>-0.793074</td>\n",
       "      <td>-0.896939</td>\n",
       "      <td>-0.073111</td>\n",
       "      <td>-0.623610</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>0.100866</td>\n",
       "      <td>-0.342673</td>\n",
       "      <td>-0.200589</td>\n",
       "      <td>0.733037</td>\n",
       "      <td>1.480513</td>\n",
       "      <td>1.142097</td>\n",
       "      <td>0.951002</td>\n",
       "      <td>0.616465</td>\n",
       "      <td>-0.587893</td>\n",
       "      <td>1.092231</td>\n",
       "      <td>1.603567</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>0.073177</td>\n",
       "      <td>-0.341805</td>\n",
       "      <td>0.562473</td>\n",
       "      <td>1.106343</td>\n",
       "      <td>0.631934</td>\n",
       "      <td>1.370695</td>\n",
       "      <td>-1.666111</td>\n",
       "      <td>-0.279598</td>\n",
       "      <td>-0.030943</td>\n",
       "      <td>1.689544</td>\n",
       "      <td>-0.623610</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>0.348158</td>\n",
       "      <td>-0.054751</td>\n",
       "      <td>0.684750</td>\n",
       "      <td>-0.604667</td>\n",
       "      <td>-0.711704</td>\n",
       "      <td>0.115418</td>\n",
       "      <td>-0.072942</td>\n",
       "      <td>-0.606834</td>\n",
       "      <td>2.045458</td>\n",
       "      <td>1.875612</td>\n",
       "      <td>1.603567</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>0.542221</td>\n",
       "      <td>0.162058</td>\n",
       "      <td>1.722973</td>\n",
       "      <td>-1.764851</td>\n",
       "      <td>1.147145</td>\n",
       "      <td>-0.617426</td>\n",
       "      <td>-1.118924</td>\n",
       "      <td>-0.627655</td>\n",
       "      <td>-0.301998</td>\n",
       "      <td>0.239208</td>\n",
       "      <td>-0.623610</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>0.323966</td>\n",
       "      <td>-0.061451</td>\n",
       "      <td>-1.795485</td>\n",
       "      <td>-0.367917</td>\n",
       "      <td>0.896467</td>\n",
       "      <td>-0.460877</td>\n",
       "      <td>0.603643</td>\n",
       "      <td>0.984109</td>\n",
       "      <td>1.649713</td>\n",
       "      <td>0.785048</td>\n",
       "      <td>-0.623610</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>0.169802</td>\n",
       "      <td>-0.115000</td>\n",
       "      <td>-1.701731</td>\n",
       "      <td>-1.001180</td>\n",
       "      <td>2.387944</td>\n",
       "      <td>0.674969</td>\n",
       "      <td>-0.332429</td>\n",
       "      <td>0.386384</td>\n",
       "      <td>1.280656</td>\n",
       "      <td>0.416720</td>\n",
       "      <td>-0.623610</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>0.102758</td>\n",
       "      <td>-0.293519</td>\n",
       "      <td>-3.398244</td>\n",
       "      <td>0.307009</td>\n",
       "      <td>-1.063514</td>\n",
       "      <td>0.303842</td>\n",
       "      <td>0.114024</td>\n",
       "      <td>-0.355136</td>\n",
       "      <td>-1.299864</td>\n",
       "      <td>0.336391</td>\n",
       "      <td>-0.623610</td>\n",
       "      <td>1.713238</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>0.085460</td>\n",
       "      <td>-0.269546</td>\n",
       "      <td>-0.770862</td>\n",
       "      <td>-1.741768</td>\n",
       "      <td>0.016137</td>\n",
       "      <td>1.491667</td>\n",
       "      <td>-1.564151</td>\n",
       "      <td>0.722848</td>\n",
       "      <td>-1.750169</td>\n",
       "      <td>0.711724</td>\n",
       "      <td>1.603567</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>0.510061</td>\n",
       "      <td>0.009035</td>\n",
       "      <td>0.294515</td>\n",
       "      <td>1.233277</td>\n",
       "      <td>0.138908</td>\n",
       "      <td>-0.275981</td>\n",
       "      <td>-0.477542</td>\n",
       "      <td>1.440887</td>\n",
       "      <td>1.981386</td>\n",
       "      <td>-0.569515</td>\n",
       "      <td>1.603567</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>0.101635</td>\n",
       "      <td>-0.196910</td>\n",
       "      <td>1.652560</td>\n",
       "      <td>0.902147</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>-1.722304</td>\n",
       "      <td>-0.400418</td>\n",
       "      <td>0.999499</td>\n",
       "      <td>-2.300320</td>\n",
       "      <td>0.559833</td>\n",
       "      <td>-0.623610</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>0.476386</td>\n",
       "      <td>0.039630</td>\n",
       "      <td>-2.324521</td>\n",
       "      <td>0.566187</td>\n",
       "      <td>0.685480</td>\n",
       "      <td>-0.098628</td>\n",
       "      <td>-0.697855</td>\n",
       "      <td>-0.772956</td>\n",
       "      <td>-0.263516</td>\n",
       "      <td>0.603167</td>\n",
       "      <td>-0.623610</td>\n",
       "      <td>1.713238</td>\n",
       "      <td>-0.561951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>0.642821</td>\n",
       "      <td>0.423562</td>\n",
       "      <td>-1.296073</td>\n",
       "      <td>-0.405887</td>\n",
       "      <td>0.115936</td>\n",
       "      <td>0.044069</td>\n",
       "      <td>0.262736</td>\n",
       "      <td>0.444955</td>\n",
       "      <td>0.615545</td>\n",
       "      <td>-0.543777</td>\n",
       "      <td>-0.623610</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>1.779513</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "8    0.570477  0.213404  0.890349 -0.147439  1.990126 -0.761451  0.339955   \n",
       "14   0.124164 -0.261967  0.898710 -0.212416  0.439933  0.477081 -0.737784   \n",
       "15   0.069134 -0.321564 -0.726354  0.019048 -0.144728  2.590595  0.276171   \n",
       "20   0.177450 -0.111869  1.260612 -0.337727  0.203715 -1.056401  0.252330   \n",
       "35   0.138507 -0.134612  0.854455  0.364880  1.131730 -1.013236  0.452629   \n",
       "38   0.120644 -0.179484  0.656598  0.069878  0.592357  0.282234 -0.945014   \n",
       "43   0.141096 -0.245467 -0.338976  0.503314 -0.816049 -0.633711  0.262273   \n",
       "52   0.129750 -0.185876 -1.550975 -0.768215 -0.131383  1.256000  1.118221   \n",
       "57   0.054896 -0.214768 -0.227539 -0.186961 -0.961218  1.451368 -0.842000   \n",
       "60   0.429239  0.050459 -1.135358  1.197269  1.182529 -0.965997 -0.730104   \n",
       "63   0.313153 -0.054648 -1.074729  0.303291 -0.701241  0.889702  1.193255   \n",
       "64   0.116480 -0.248494  0.325751  0.116851  0.352193  0.321785 -0.090461   \n",
       "70   0.166282 -0.295967 -0.477405  1.224248 -2.444276 -1.277340 -0.477845   \n",
       "72  -1.184669 -2.606315 -0.798833 -1.509623  0.663433 -0.166495 -1.843152   \n",
       "73   0.405787 -0.035257  0.907566  1.506355 -1.578018  0.780546 -0.270929   \n",
       "79  -0.916263 -3.188031 -2.000470  0.187104 -0.624058 -0.773404  1.014346   \n",
       "86   0.533947  0.017610  1.321926 -0.344979 -1.306716 -0.991680  0.303015   \n",
       "94   0.057157 -0.482965 -2.347794 -0.215441  0.818455  0.080684  0.232333   \n",
       "97   0.176835 -0.100631 -0.094519  0.916606  2.218899 -0.904311 -0.927912   \n",
       "102  0.113415 -0.281718 -0.519737 -1.488765  0.642339  0.944480  1.146044   \n",
       "112  0.098399 -0.329514  0.721658  0.597793 -2.633525 -0.702436  0.251464   \n",
       "122  0.173240 -0.105694  0.441679  0.058440 -1.951499 -0.203288  0.405258   \n",
       "124  0.364203  0.049917 -0.197488 -0.783216 -1.426864  0.319758 -0.119460   \n",
       "126  0.150924 -0.142967  0.384473 -0.672574 -1.060597 -0.517821  1.546843   \n",
       "130  0.106728 -0.264637 -0.932090 -2.582287  0.344004  0.065492  0.327097   \n",
       "139  0.126743 -0.370473 -0.999558  0.210221 -0.471062 -0.270545 -1.104174   \n",
       "154  0.468681  0.050437 -1.355537  0.811363 -1.344335  0.449845 -0.908124   \n",
       "174 -1.222457 -2.835874  0.673598  1.476609 -0.711814  1.176995  0.549289   \n",
       "176  0.098747 -0.302331 -0.063855  1.978516  0.195140 -0.543343  0.433188   \n",
       "181  0.543454  0.062956 -0.896043  2.059291 -0.529956  0.216319 -0.551943   \n",
       "182  0.097527 -0.243130  0.441559  0.351539 -0.445331 -0.650953 -0.734543   \n",
       "189  0.636393  0.365879 -0.143797 -0.246778  1.655661 -1.857235  1.343687   \n",
       "208  0.116067 -0.201666  0.334041  0.555705 -0.878623 -0.042333  0.399595   \n",
       "209  0.426195  0.077380 -0.952572 -0.055420 -0.948238  0.365358 -0.910338   \n",
       "242  0.107171 -0.292872 -0.603940  0.536668 -0.075732  1.787608 -0.191817   \n",
       "269  0.132037 -0.151017 -0.863842 -2.132282  0.727950  1.337556 -0.696062   \n",
       "293  0.460032  0.072800  0.597998  0.130124 -0.344579  0.103229  1.487559   \n",
       "303  0.335006 -0.076516 -0.228675  0.005427  1.023121  1.418721 -0.596386   \n",
       "305 -0.674799 -3.314446  1.569892 -1.165726  0.511096  0.427927  1.421894   \n",
       "321 -1.177411 -2.842691 -0.817647  0.420797 -0.436232  0.764009  0.922755   \n",
       "328  0.139323 -0.169897  0.736484 -0.771523  0.343673 -0.376998 -0.565768   \n",
       "336  0.100866 -0.342673 -0.200589  0.733037  1.480513  1.142097  0.951002   \n",
       "348  0.073177 -0.341805  0.562473  1.106343  0.631934  1.370695 -1.666111   \n",
       "360  0.348158 -0.054751  0.684750 -0.604667 -0.711704  0.115418 -0.072942   \n",
       "363  0.542221  0.162058  1.722973 -1.764851  1.147145 -0.617426 -1.118924   \n",
       "364  0.323966 -0.061451 -1.795485 -0.367917  0.896467 -0.460877  0.603643   \n",
       "370  0.169802 -0.115000 -1.701731 -1.001180  2.387944  0.674969 -0.332429   \n",
       "371  0.102758 -0.293519 -3.398244  0.307009 -1.063514  0.303842  0.114024   \n",
       "376  0.085460 -0.269546 -0.770862 -1.741768  0.016137  1.491667 -1.564151   \n",
       "382  0.510061  0.009035  0.294515  1.233277  0.138908 -0.275981 -0.477542   \n",
       "407  0.101635 -0.196910  1.652560  0.902147  0.260000 -1.722304 -0.400418   \n",
       "413  0.476386  0.039630 -2.324521  0.566187  0.685480 -0.098628 -0.697855   \n",
       "422  0.642821  0.423562 -1.296073 -0.405887  0.115936  0.044069  0.262736   \n",
       "\n",
       "           7         8         9         10        11        12   13   14  \n",
       "8   -0.810964  1.438997  0.287414  1.603567 -0.583690 -0.561951  0.0  1.0  \n",
       "14   1.681344 -0.114237  1.085141 -0.623610 -0.583690  1.779513  1.0  0.0  \n",
       "15  -1.268726  0.674310 -0.155300 -0.623610 -0.583690  1.779513  1.0  0.0  \n",
       "20  -1.398638 -0.140565 -0.049648  1.603567 -0.583690 -0.561951  1.0  0.0  \n",
       "35   1.127784 -1.360652 -0.704170  1.603567 -0.583690 -0.561951  1.0  0.0  \n",
       "38  -1.551942 -1.379001 -3.006900  1.603567 -0.583690 -0.561951  1.0  0.0  \n",
       "43  -2.160652 -0.125377 -1.194919 -0.623610 -0.583690 -0.561951  1.0  0.0  \n",
       "52  -0.086961 -0.370840 -1.954281  1.603567 -0.583690 -0.561951  1.0  0.0  \n",
       "57  -0.189125 -0.707262 -1.409180  1.603567 -0.583690 -0.561951  1.0  0.0  \n",
       "60  -0.844970  1.649886  1.026833 -0.623610  1.713238 -0.561951  0.0  1.0  \n",
       "63   0.640984  0.888495 -0.468003 -0.623610 -0.583690 -0.561951  0.0  1.0  \n",
       "64   1.615395 -0.562830 -0.808936 -0.623610 -0.583690 -0.561951  1.0  0.0  \n",
       "70   0.347005 -1.055721 -0.323697  1.603567 -0.583690 -0.561951  1.0  0.0  \n",
       "72   0.076270  0.413933  0.227159 -0.623610 -0.583690  1.779513  0.0  1.0  \n",
       "73  -0.879325  1.495713 -0.150713  1.603567 -0.583690 -0.561951  0.0  1.0  \n",
       "79   0.219134 -0.782451 -0.894581 -0.623610  1.713238 -0.561951  0.0  1.0  \n",
       "86  -1.274544  0.989536 -2.138604 -0.623610 -0.583690  1.779513  0.0  1.0  \n",
       "94   0.854870 -0.921479 -0.120332 -0.623610 -0.583690  1.779513  1.0  0.0  \n",
       "97  -0.344575 -0.115748 -0.965014 -0.623610 -0.583690  1.779513  1.0  0.0  \n",
       "102 -1.430010  0.607518  0.045193 -0.623610  1.713238 -0.561951  1.0  0.0  \n",
       "112 -0.450606 -2.243365  0.124079 -0.623610  1.713238 -0.561951  1.0  0.0  \n",
       "122 -0.669158 -0.593083 -1.728092 -0.623610 -0.583690 -0.561951  1.0  0.0  \n",
       "124  0.897862  0.598068 -0.779030 -0.623610 -0.583690 -0.561951  0.0  1.0  \n",
       "126 -0.803415 -0.049176 -0.272014 -0.623610 -0.583690  1.779513  1.0  0.0  \n",
       "130  0.181750  1.450440 -1.140263 -0.623610 -0.583690 -0.561951  1.0  0.0  \n",
       "139 -1.398525  0.091643 -0.710227 -0.623610  1.713238 -0.561951  1.0  0.0  \n",
       "154  0.483980  1.713257  0.983963 -0.623610 -0.583690 -0.561951  0.0  1.0  \n",
       "174  1.215586  0.006918  0.544925 -0.623610 -0.583690 -0.561951  0.0  1.0  \n",
       "176  0.044345 -0.088830  1.739541 -0.623610  1.713238 -0.561951  1.0  0.0  \n",
       "181 -2.190064  0.619470  0.036925 -0.623610  1.713238 -0.561951  0.0  1.0  \n",
       "182  1.715815  0.444283 -0.591329 -0.623610  1.713238 -0.561951  1.0  0.0  \n",
       "189  0.394545 -0.504834  0.631005 -0.623610  1.713238 -0.561951  0.0  1.0  \n",
       "208  0.338007  0.376224 -0.751949  1.603567 -0.583690 -0.561951  1.0  0.0  \n",
       "209  0.739330 -0.072793  0.490103  1.603567 -0.583690 -0.561951  0.0  1.0  \n",
       "242 -1.507226  0.606685  0.319507 -0.623610 -0.583690  1.779513  1.0  0.0  \n",
       "269  0.359005 -0.173122 -0.217989 -0.623610  1.713238 -0.561951  1.0  0.0  \n",
       "293  1.079272  0.513538  0.041187 -0.623610 -0.583690 -0.561951  0.0  1.0  \n",
       "303 -0.101274  1.036712  0.625963 -0.623610 -0.583690  1.779513  0.0  1.0  \n",
       "305 -0.085293 -0.461342 -0.234540  1.603567 -0.583690 -0.561951  0.0  1.0  \n",
       "321 -0.511361  0.195164 -0.895794 -0.623610 -0.583690  1.779513  0.0  1.0  \n",
       "328 -0.793074 -0.896939 -0.073111 -0.623610 -0.583690 -0.561951  1.0  0.0  \n",
       "336  0.616465 -0.587893  1.092231  1.603567 -0.583690 -0.561951  1.0  0.0  \n",
       "348 -0.279598 -0.030943  1.689544 -0.623610 -0.583690 -0.561951  1.0  0.0  \n",
       "360 -0.606834  2.045458  1.875612  1.603567 -0.583690 -0.561951  0.0  1.0  \n",
       "363 -0.627655 -0.301998  0.239208 -0.623610 -0.583690 -0.561951  0.0  1.0  \n",
       "364  0.984109  1.649713  0.785048 -0.623610 -0.583690 -0.561951  0.0  1.0  \n",
       "370  0.386384  1.280656  0.416720 -0.623610 -0.583690 -0.561951  1.0  0.0  \n",
       "371 -0.355136 -1.299864  0.336391 -0.623610  1.713238 -0.561951  1.0  0.0  \n",
       "376  0.722848 -1.750169  0.711724  1.603567 -0.583690 -0.561951  1.0  0.0  \n",
       "382  1.440887  1.981386 -0.569515  1.603567 -0.583690 -0.561951  0.0  1.0  \n",
       "407  0.999499 -2.300320  0.559833 -0.623610 -0.583690 -0.561951  1.0  0.0  \n",
       "413 -0.772956 -0.263516  0.603167 -0.623610  1.713238 -0.561951  0.0  1.0  \n",
       "422  0.444955  0.615545 -0.543777 -0.623610 -0.583690  1.779513  0.0  1.0  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.iloc[mis] #DataFrame of all misclassified records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at the feature coefficients(of first and second feature) and the predicted target class it is evident why the records were misclassified. We see that the when the first feature value in any record is hugely positive the predicted y is 1 and if the second feature value in any record is hugely negative the predicted y is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " a) To your team: why it is the best model? Give an example of a data point that model doesnt do well and explain the characteristics of that observation. How- do you see the limitations of your model? How do you plan to improve it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ans. Take example of a data which has been severely misclassified and look at its features. Try to form a narrative as to why the data point was misclassified looking the model feature importance and individual features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " b 1).  A few days after you finish the initial model, you learn new information that the value of the X2 feature makes sense only if it is positive (i.e. X2 > 0 should be in the model instead of original X2). Show how you would address this issue.\n",
    "#### Ans. check how many data points in the original data have X2 < 0. If the number is very less we can retrain the model by dropping those data points where X2<0. If almost 50% of the data have X2 < 0 then the variable would be no longer reliable since business intuition tell us X2 > 0. In this case there is no other way than to drop the variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " b 2). Later, you additionally learn that X4 shouldve been used as an indicator variable (e.g. X4 that is greater than a certain threshold should be treated differently than X4 values below that threshold). However, you dont know the value of threshold. Adapt your code to support a systematic way of finding threshold which leads to improved model performance.\n",
    "#### Ans. Approach 1. Iterate through all the records in x4, in each iteration use the corresponding x4 value as the threshold for predicting the target class(if the x4 value is greater than the current threshold assign 1 else 0). Calculate the F1 score for each of these thresholds, the threshold with the highest F1 score will be our final threshold.  \n",
    "    \n",
    "#### Approach 2. Build a single variable decision tree having 2 leaves with the X4 variable as the predictor and y as the target variable on the train data. Since the decision tree will select the value of the continuous variable which results in the most homogenous split,  the threshold where the tree splits can be the threshold for converting X4 to an indicator variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.922338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.781739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.120545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.961795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.113519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   y        x4\n",
       "0  0 -0.922338\n",
       "1  0 -1.781739\n",
       "2  1 -1.120545\n",
       "3  0  0.961795\n",
       "4  1 -1.113519"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./DataSet2.csv')\n",
    "df[['y', 'x4']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                       max_features=None, max_leaf_nodes=2,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=0, splitter='best')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[['x4']], df[['y']])\n",
    "\n",
    "clf = DecisionTreeClassifier(max_leaf_nodes=2, random_state=0)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    # return math.sqrt(x)\n",
    "    return 'r' if x == 1 else 'g'\n",
    "\n",
    "\n",
    "color_mask = np.array([f(xi) for xi in y_train.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.741030514240265"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.tree_.threshold[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x23dfb0e5160>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_train, np.ones(len(X_train)), c = color_mask) #checking which records have class 0 and which have class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "tree.plot_tree(clf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b 3).\tNow that you realize feature engineering is a frequent need for improving our models, you want to create a general framework to tackle such situations (e.g. conversion to indicators, defining certain thresholds, etc.). What function/module do you develop to enable user-defined column conversions for given thresholds? Suppose thresholds are given to you as arguments to your function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, th):\n",
    "    return 1 if x >= th else 0 #return 1 if value grater than equal to threshold else 0\n",
    "\n",
    "def continous_to_indicator(df, thresholds):\n",
    "    for index, threshold in  enumerate(thresholds):\n",
    "        if threshold is not None:\n",
    "            df.iloc[:, index] = np.array([f(xi, threshold) for xi in df.values[:,index]]) #iterate through the list and for each threshold which is not None change the column accordingly \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp=df_new.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>x11</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.375866</td>\n",
       "      <td>0.427942</td>\n",
       "      <td>-0.922338</td>\n",
       "      <td>0.210758</td>\n",
       "      <td>0.109015</td>\n",
       "      <td>0.621001</td>\n",
       "      <td>-0.444421</td>\n",
       "      <td>0.089970</td>\n",
       "      <td>-0.707711</td>\n",
       "      <td>0.473700</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.047819</td>\n",
       "      <td>0.115627</td>\n",
       "      <td>-1.781739</td>\n",
       "      <td>-0.272785</td>\n",
       "      <td>0.392783</td>\n",
       "      <td>1.094168</td>\n",
       "      <td>-0.975254</td>\n",
       "      <td>-0.353424</td>\n",
       "      <td>0.145543</td>\n",
       "      <td>-0.064961</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.372868</td>\n",
       "      <td>-0.263291</td>\n",
       "      <td>-1.120545</td>\n",
       "      <td>-0.773828</td>\n",
       "      <td>0.830072</td>\n",
       "      <td>-1.727836</td>\n",
       "      <td>1.323876</td>\n",
       "      <td>-1.587291</td>\n",
       "      <td>-0.024916</td>\n",
       "      <td>0.082491</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.059598</td>\n",
       "      <td>0.270797</td>\n",
       "      <td>0.961795</td>\n",
       "      <td>-1.804197</td>\n",
       "      <td>2.931330</td>\n",
       "      <td>1.891656</td>\n",
       "      <td>0.094252</td>\n",
       "      <td>-0.873467</td>\n",
       "      <td>-1.217680</td>\n",
       "      <td>-1.848046</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.616319</td>\n",
       "      <td>0.291275</td>\n",
       "      <td>-1.113519</td>\n",
       "      <td>0.626864</td>\n",
       "      <td>-0.287989</td>\n",
       "      <td>-0.842649</td>\n",
       "      <td>-0.947257</td>\n",
       "      <td>1.198215</td>\n",
       "      <td>0.972420</td>\n",
       "      <td>-1.054313</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x2        x3        x4        x5        x6        x7        x8  \\\n",
       "0 -0.375866  0.427942 -0.922338  0.210758  0.109015  0.621001 -0.444421   \n",
       "1  0.047819  0.115627 -1.781739 -0.272785  0.392783  1.094168 -0.975254   \n",
       "2  0.372868 -0.263291 -1.120545 -0.773828  0.830072 -1.727836  1.323876   \n",
       "3  0.059598  0.270797  0.961795 -1.804197  2.931330  1.891656  0.094252   \n",
       "4  0.616319  0.291275 -1.113519  0.626864 -0.287989 -0.842649 -0.947257   \n",
       "\n",
       "         x9       x10       x11  b  c  d  \n",
       "0  0.089970 -0.707711  0.473700  0  0  0  \n",
       "1 -0.353424  0.145543 -0.064961  1  0  0  \n",
       "2 -1.587291 -0.024916  0.082491  0  0  1  \n",
       "3 -0.873467 -1.217680 -1.848046  0  1  0  \n",
       "4  1.198215  0.972420 -1.054313  0  0  1  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head() #old DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>x11</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.375866</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.922338</td>\n",
       "      <td>0</td>\n",
       "      <td>0.109015</td>\n",
       "      <td>0.621001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.047819</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.781739</td>\n",
       "      <td>0</td>\n",
       "      <td>0.392783</td>\n",
       "      <td>1.094168</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.372868</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.120545</td>\n",
       "      <td>0</td>\n",
       "      <td>0.830072</td>\n",
       "      <td>-1.727836</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.059598</td>\n",
       "      <td>1</td>\n",
       "      <td>0.961795</td>\n",
       "      <td>0</td>\n",
       "      <td>2.931330</td>\n",
       "      <td>1.891656</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.616319</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.113519</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.287989</td>\n",
       "      <td>-0.842649</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x2  x3        x4  x5        x6        x7  x8  x9  x10  x11  b  c  d\n",
       "0 -0.375866   1 -0.922338   0  0.109015  0.621001   0   0    0    0  0  0  0\n",
       "1  0.047819   0 -1.781739   0  0.392783  1.094168   0   0    0    0  1  0  0\n",
       "2  0.372868   0 -1.120545   0  0.830072 -1.727836   0   0    0    0  0  0  1\n",
       "3  0.059598   1  0.961795   0  2.931330  1.891656   0   0    0    0  0  1  0\n",
       "4  0.616319   1 -1.113519   0 -0.287989 -0.842649   0   0    1    0  0  0  1"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func=continous_to_indicator(df_temp, [None,0.2,None,1,None,None,2,3,0.4,0.5,0.6,0.8,0.9])\n",
    "func.head() #the modified DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
